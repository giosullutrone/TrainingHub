{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, PreTrainedTokenizer\n",
    "from torch import bfloat16\n",
    "\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B\", torch_dtype=bfloat16, device_map=\"cpu\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.3\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = [{\"role\": \"user\", \"content\": \"What's the weather like in Paris?\"}, \n",
    "                {\"role\": \"assistant\", \"content\": \"gg\"}]\n",
    "tools = None\n",
    "\n",
    "tool_use_prompt = tokenizer.apply_chat_template(\n",
    "            conversation,\n",
    "            tools=tools,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from recipes import DatasetRecipe\n",
    "def get_response_template(tokenizer: PreTrainedTokenizer, dataset_recipe: DatasetRecipe) -> str:\n",
    "    if dataset_recipe.dataset_system_message is not None: \n",
    "        conversation = [{\"role\": \"system\", \"content\": dataset_recipe.dataset_system_message}]\n",
    "    else: conversation = []\n",
    "    conversation += [\n",
    "        {\"role\": \"user\", \"content\": \"dummy\"}\n",
    "    ]\n",
    "    \n",
    "    dummy_no_generation_prompt = tokenizer.apply_chat_template(\n",
    "            conversation,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=False,\n",
    "    )\n",
    "    \n",
    "    dummy_generation_prompt = tokenizer.apply_chat_template(\n",
    "            conversation,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True,\n",
    "    )\n",
    "    \n",
    "    for index, (c1, c2) in enumerate(zip(dummy_no_generation_prompt, \n",
    "                                         dummy_generation_prompt)):\n",
    "        if c1 != c2: return dummy_generation_prompt[index:]\n",
    "    else: return dummy_generation_prompt[len(dummy_no_generation_prompt):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>assistant\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from recipes.datasets.PostOCRCorrectionDatasetRecipe import PostOCRCorrectionDatasetRecipe\n",
    "\n",
    "print(get_response_template(tokenizer, PostOCRCorrectionDatasetRecipe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.chat_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<|im_start|>user\\nWhat's the weather like in Paris?<|im_end|>\\n<|im_start|>assistant\\ngg<|im_end|>\\n\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_use_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bos_token': '<|begin_of_text|>', 'eos_token': '<|end_of_text|>'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Gio\\miniconda3\\envs\\prompteffectiveness\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import fields, field, dataclass, asdict\n",
    "import argparse\n",
    "from typing import Union\n",
    "from ast import literal_eval\n",
    "from typing import Dict, Union, Optional\n",
    "from datasets import DatasetDict, Dataset, IterableDatasetDict, IterableDataset\n",
    "from recipes.Recipe import Recipe\n",
    "from cookbooks import DATASET_COOKBOOK\n",
    "\n",
    "@dataclass\n",
    "class DatasetRecipe(Recipe):\n",
    "    \"\"\"Kwargs to give to the \"load_dataset\" function from \"datasets\" module\"\"\"\n",
    "    dataset_load: Optional[dict] = field(default=None, metadata={\"description\": \"test\"})\n",
    "    response_template: Optional[str] = field(default=None)\n",
    "\n",
    "    def preprocess_dataset(self, dataset: Union[DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None]) -> Union[DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None]:\n",
    "        return dataset\n",
    "    \n",
    "    def preprocess_function(self, sample: Dict, examples: Union[DatasetDict, Dataset, IterableDatasetDict, IterableDataset, dict, None]) -> Dict:\n",
    "        return sample\n",
    "\n",
    "class ModelRecipe(Recipe):\n",
    "    model_load: Optional[dict] = None\n",
    "    model_config: Optional[dict] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_dataclasses(dataclass_list):\n",
    "    # Convert each dataclass in the list to a dictionary with its metadata\n",
    "    dicts_with_metadata = []\n",
    "    for cls in dataclass_list:\n",
    "        fis = {fi.name: fi.type for fi in asdict(cls.__annotations__.values())}\n",
    "        metadata = {\"created_at\": getattr(cls, \"created_at\", 0)}\n",
    "        dicts_with_metadata.append({**fis, **metadata})\n",
    "\n",
    "    # Merge the dictionaries together\n",
    "    merged_dict = {}\n",
    "    for d in dicts_with_metadata:\n",
    "        merged_dict.update(d)\n",
    "\n",
    "    return merged_dict\n",
    "\n",
    "# Define a function to convert the merged dictionary back into a dataclass\n",
    "def dict_to_dataclass(cls, data):\n",
    "    annotations = {k: v for k, v in cls.__annotations__.items() if k not in [\"created_at\", \"source\"]}\n",
    "    data = {**annotations, **data}\n",
    "    return cls(**data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'DatasetRecipe' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmerge_dataclasses\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mDatasetRecipe\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mModelRecipe\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[12], line 5\u001b[0m, in \u001b[0;36mmerge_dataclasses\u001b[1;34m(dataclass_list)\u001b[0m\n\u001b[0;32m      3\u001b[0m dicts_with_metadata \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01min\u001b[39;00m dataclass_list:\n\u001b[1;32m----> 5\u001b[0m     fis \u001b[38;5;241m=\u001b[39m {fi\u001b[38;5;241m.\u001b[39mname: fi\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;28;01mfor\u001b[39;00m fi \u001b[38;5;129;01min\u001b[39;00m asdict(\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__annotations__\u001b[39m\u001b[38;5;241m.\u001b[39mvalues()}\n\u001b[0;32m      6\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcreated_at\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcreated_at\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0\u001b[39m)}\n\u001b[0;32m      7\u001b[0m     dicts_with_metadata\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfis, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmetadata})\n",
      "\u001b[1;31mTypeError\u001b[0m: 'DatasetRecipe' object is not callable"
     ]
    }
   ],
   "source": [
    "merge_dataclasses([DatasetRecipe(), ModelRecipe()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'argparse' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 71\u001b[0m\n\u001b[0;32m     67\u001b[0m             parser\u001b[38;5;241m.\u001b[39madd_argument(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfield\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39mfield_type, default\u001b[38;5;241m=\u001b[39mfield_default, help\u001b[38;5;241m=\u001b[39mfield_help)\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[1;32m---> 71\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mgenerate_argparser_from_recipe\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mDatasetRecipe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mModelRecipe\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mprint_help())\n",
      "Cell \u001b[1;32mIn[29], line 48\u001b[0m, in \u001b[0;36mgenerate_argparser_from_recipe\u001b[1;34m(recipe_dataclasses, description)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_argparser_from_recipe\u001b[39m(recipe_dataclasses: List[\u001b[38;5;28mobject\u001b[39m], description: \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m     38\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;124;03m    Generate an argparse.ArgumentParser based on the fields of a dataclass.\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;124;03m        argparse.ArgumentParser: Argument parser populated with dataclass fields.\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 48\u001b[0m     parser \u001b[38;5;241m=\u001b[39m \u001b[43margparse\u001b[49m\u001b[38;5;241m.\u001b[39mArgumentParser(description\u001b[38;5;241m=\u001b[39mdescription)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m recipe_dataclass \u001b[38;5;129;01min\u001b[39;00m recipe_dataclasses:\n\u001b[0;32m     51\u001b[0m         parser\u001b[38;5;241m.\u001b[39madd_argument(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcamel_to_snake(recipe_dataclass\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \n\u001b[0;32m     52\u001b[0m                             \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m, default\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \n\u001b[0;32m     53\u001b[0m                             help\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRecipe\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms name from cookbook\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'argparse' is not defined"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------------------------\n",
    "# Here we define all the parameters for our training procedure (\"train_cli.py\")\n",
    "# Each parameter is a field. For each of them we specify the type, default and some useful metadata.\n",
    "# \n",
    "# Metadata:\n",
    "#   - `description` (str): Description of the field. Used both as documentation and also by the argument parser\n",
    "#                          as information to show when calling `--help` on `train_cli.py` \n",
    "#   - `required` (bool): Whether it is required for running `train_cli.py`. \n",
    "#                        Note: This is done instead of removing `None` from (Union[..., None]) so that we can use also\n",
    "#                              use a starting config file for common parameters (See utils.parsers.get_config_from_argparser for the code)\n",
    "#   - `recipe_keywords` (List[str]): List of optional fields' names to use to create the recipe in case it was specified as a string.\n",
    "#                           For example --model_recipe \"MistralModelRecipe\" will also use `model_load` and `model_config` fields for initialization.\n",
    "#   - `cookbook` (CookBook): Cookbook to use to get the `recipe` if it was specified by string. \n",
    "#                            For example --model_recipe \"MistralModelRecipe\", the string \"MistralModelRecipe\" would be used to get the \n",
    "#                            class from the given cookbook.\n",
    "# --------------------------------------------------------------------------\n",
    "from typing import List, Tuple, Any\n",
    "\n",
    "def camel_to_snake(camel_string):\n",
    "    \"\"\"\n",
    "    Convert camelCase string to snake_case string.\n",
    "\n",
    "    Args:\n",
    "        camel_string (str): The input string in camelCase format.\n",
    "\n",
    "    Returns:\n",
    "        str: The output string in snake_case format.\n",
    "    \"\"\"\n",
    "    result = ''\n",
    "    for i, char in enumerate(camel_string):\n",
    "        if char.isupper() and i > 0:\n",
    "            result += '_' + char.lower()\n",
    "        else:\n",
    "            result += char.lower()\n",
    "    return result\n",
    "\n",
    "def generate_argparser_from_recipe(recipe_dataclasses: List[object], description: str):\n",
    "    \"\"\"\n",
    "    Generate an argparse.ArgumentParser based on the fields of a dataclass.\n",
    "\n",
    "    Args:\n",
    "        dataclass_type: Type of the dataclass.\n",
    "        description (str): Description for the argument parser.\n",
    "\n",
    "    Returns:\n",
    "        argparse.ArgumentParser: Argument parser populated with dataclass fields.\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser(description=description)\n",
    "\n",
    "    for recipe_dataclass in recipe_dataclasses:\n",
    "        parser.add_argument(f\"--{camel_to_snake(recipe_dataclass.__name__)}\", \n",
    "                            type=str, default=None, \n",
    "                            help=\"Recipe's name from cookbook\")\n",
    "        \n",
    "        # Iterate through fields of the dataclass\n",
    "        for field in fields(recipe_dataclass):\n",
    "            field_type = field.type\n",
    "            # Handle Optional types\n",
    "            if hasattr(field_type, \"__origin__\") and field_type.__origin__ is Optional:\n",
    "                valid_types = [t for t in field_type.__args__ if t in (str, int, dict, float)]\n",
    "                field_type = valid_types[0]\n",
    "                # If type is dict, we make the argparse evaluate the string\n",
    "                # This converts the string '{\"example\": \"example value\"}' into the final dict\n",
    "                if field_type == dict: field_type = literal_eval\n",
    "            field_default = field.default if field.default is not None else None\n",
    "            field_help = field.metadata.get(\"description\", \"\")\n",
    "            parser.add_argument(f\"--{field.name}\", type=field_type, default=field_default, help=field_help)\n",
    "\n",
    "    return parser\n",
    "\n",
    "print(generate_argparser_from_recipe([DatasetRecipe, ModelRecipe], \"\").print_help())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataset_load': typing.Optional[dict],\n",
       " 'response_template': typing.Optional[str]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DatasetRecipe.__annotations__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import asdict, dataclass\n",
    "from pprint import pprint\n",
    "\n",
    "def join_dicts(priority: Optional[dict], secondary: Optional[dict]) -> Dict:\n",
    "    if not secondary: secondary = {}\n",
    "    if not priority: priority = {}\n",
    "    return {**secondary, **priority}\n",
    "\n",
    "@dataclass\n",
    "class DatasetRecipe(Recipe):\n",
    "    \"\"\"Kwargs to give to the \"load_dataset\" function from \"datasets\" module\"\"\"\n",
    "    dataset_load: Optional[dict] = field(default=None, metadata={\"description\": \"test\"})\n",
    "    response_template: Optional[str] = field(default=None)\n",
    "\n",
    "@dataclass\n",
    "class ModelRecipe(Recipe):\n",
    "    model_load: Optional[dict] = None\n",
    "    model_config: Optional[dict] = None\n",
    "\n",
    "# @dataclass\n",
    "# class RecipeMerge:\n",
    "#     def _update_fields(self, field_dict: dict):\n",
    "#         self.__dataclass_fields__.update(field_dict)\n",
    "#     \n",
    "#     def __init__(self, recipe_dataclasses: List[dataclass]):\n",
    "#         for recipe_dataclass in recipe_dataclasses:\n",
    "#             self._update_fields(recipe_dataclass.__dataclass_fields__)\n",
    "#             \n",
    "#             # For some reason the field does not appear unless we modify an already existing one\n",
    "#             fi = fields(recipe_dataclass)[0]\n",
    "#             fi.name = camel_to_snake(recipe_dataclass.__name__)\n",
    "#             fi.default = None; fi.type = str\n",
    "#             fi.metadata = {\"required\": True, \"cookbook\": \"\", \"recipe_keywords\": fields(recipe_dataclass)}\n",
    "#             self._update_fields({camel_to_snake(recipe_dataclass.__name__): fi})\n",
    "# \n",
    "# t = RecipeMerge([DatasetRecipe, ModelRecipe])\n",
    "\n",
    "# pprint(t.__dataclass_fields__)\n",
    "\n",
    "# for fi in fields(t):\n",
    "#     print(fi.name, fi.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Field(name='dataset_load',type=Field(name='dataset_load',type=typing.Optional[dict],default=None,default_factory=<dataclasses._MISSING_TYPE object at 0x0000014DAAA6AB00>,init=True,repr=True,hash=None,compare=True,metadata=mappingproxy({'description': 'test'}),kw_only=False,_field_type=_FIELD),default=<dataclasses._MISSING_TYPE object at 0x0000014DAAA6AB00>,default_factory=<dataclasses._MISSING_TYPE object at 0x0000014DAAA6AB00>,init=True,repr=True,hash=None,compare=True,metadata=mappingproxy({}),kw_only=False,_field_type=_FIELD),\n",
       " Field(name='response_template',type=Field(name='response_template',type=typing.Optional[str],default=None,default_factory=<dataclasses._MISSING_TYPE object at 0x0000014DAAA6AB00>,init=True,repr=True,hash=None,compare=True,metadata=mappingproxy({}),kw_only=False,_field_type=_FIELD),default=<dataclasses._MISSING_TYPE object at 0x0000014DAAA6AB00>,default_factory=<dataclasses._MISSING_TYPE object at 0x0000014DAAA6AB00>,init=True,repr=True,hash=None,compare=True,metadata=mappingproxy({}),kw_only=False,_field_type=_FIELD),\n",
       " Field(name='dataset_recipe',type=Field(name=None,type=None,default=None,default_factory=<dataclasses._MISSING_TYPE object at 0x0000014DAAA6AB00>,init=True,repr=True,hash=None,compare=True,metadata=mappingproxy({'required': True, 'cookbook': '', 'recipe_keywords': (Field(name='dataset_load',type=typing.Optional[dict],default=None,default_factory=<dataclasses._MISSING_TYPE object at 0x0000014DAAA6AB00>,init=True,repr=True,hash=None,compare=True,metadata=mappingproxy({'description': 'test'}),kw_only=False,_field_type=_FIELD), Field(name='response_template',type=typing.Optional[str],default=None,default_factory=<dataclasses._MISSING_TYPE object at 0x0000014DAAA6AB00>,init=True,repr=True,hash=None,compare=True,metadata=mappingproxy({}),kw_only=False,_field_type=_FIELD))}),kw_only=<dataclasses._MISSING_TYPE object at 0x0000014DAAA6AB00>,_field_type=None),default=<dataclasses._MISSING_TYPE object at 0x0000014DAAA6AB00>,default_factory=<dataclasses._MISSING_TYPE object at 0x0000014DAAA6AB00>,init=True,repr=True,hash=None,compare=True,metadata=mappingproxy({}),kw_only=False,_field_type=_FIELD),\n",
       " Field(name='model_load',type=Field(name='model_load',type=typing.Optional[dict],default=None,default_factory=<dataclasses._MISSING_TYPE object at 0x0000014DAAA6AB00>,init=True,repr=True,hash=None,compare=True,metadata=mappingproxy({}),kw_only=False,_field_type=_FIELD),default=<dataclasses._MISSING_TYPE object at 0x0000014DAAA6AB00>,default_factory=<dataclasses._MISSING_TYPE object at 0x0000014DAAA6AB00>,init=True,repr=True,hash=None,compare=True,metadata=mappingproxy({}),kw_only=False,_field_type=_FIELD),\n",
       " Field(name='model_config',type=Field(name='model_config',type=typing.Optional[dict],default=None,default_factory=<dataclasses._MISSING_TYPE object at 0x0000014DAAA6AB00>,init=True,repr=True,hash=None,compare=True,metadata=mappingproxy({}),kw_only=False,_field_type=_FIELD),default=<dataclasses._MISSING_TYPE object at 0x0000014DAAA6AB00>,default_factory=<dataclasses._MISSING_TYPE object at 0x0000014DAAA6AB00>,init=True,repr=True,hash=None,compare=True,metadata=mappingproxy({}),kw_only=False,_field_type=_FIELD),\n",
       " Field(name='model_recipe',type=Field(name=None,type=None,default=None,default_factory=<dataclasses._MISSING_TYPE object at 0x0000014DAAA6AB00>,init=True,repr=True,hash=None,compare=True,metadata=mappingproxy({'required': True, 'cookbook': '', 'recipe_keywords': (Field(name='model_load',type=typing.Optional[dict],default=None,default_factory=<dataclasses._MISSING_TYPE object at 0x0000014DAAA6AB00>,init=True,repr=True,hash=None,compare=True,metadata=mappingproxy({}),kw_only=False,_field_type=_FIELD), Field(name='model_config',type=typing.Optional[dict],default=None,default_factory=<dataclasses._MISSING_TYPE object at 0x0000014DAAA6AB00>,init=True,repr=True,hash=None,compare=True,metadata=mappingproxy({}),kw_only=False,_field_type=_FIELD))}),kw_only=<dataclasses._MISSING_TYPE object at 0x0000014DAAA6AB00>,_field_type=None),default=<dataclasses._MISSING_TYPE object at 0x0000014DAAA6AB00>,default_factory=<dataclasses._MISSING_TYPE object at 0x0000014DAAA6AB00>,init=True,repr=True,hash=None,compare=True,metadata=mappingproxy({}),kw_only=False,_field_type=_FIELD))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dataclasses import make_dataclass\n",
    "\n",
    "def get_config(recipe_dataclasses: List[Tuple[dataclass, bool, Cookbook]]):\n",
    "    fis = []\n",
    "    for recipe, required, cookbook in [(DatasetRecipe, True, DATASET_COOKBOOK), ModelRecipe]:\n",
    "        fis += [(x.name, x) for x in fields(recipe)]\n",
    "        fis += [(camel_to_snake(recipe.__name__), field(default=None, metadata = {\"required\": required, \n",
    "                                                                            \"cookbook\": cookbook, \n",
    "                                                                            \"recipe_keywords\": fields(recipe)}))]\n",
    "    make_dataclass(\"Config\", fields=fis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Gio\\miniconda3\\envs\\orion\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass, fields, make_dataclass, field\n",
    "import os\n",
    "from recipes import DatasetRecipe\n",
    "from recipes import ModelRecipe\n",
    "from recipes import TokenizerRecipe\n",
    "from recipes import PeftRecipe\n",
    "from recipes import QuantizationRecipe\n",
    "from recipes import TrainRecipe\n",
    "\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    dataset_recipe: DatasetRecipe\n",
    "    model_recipe: ModelRecipe\n",
    "    tokenizer_recipe: TokenizerRecipe\n",
    "    peft_recipe: PeftRecipe\n",
    "    quantization_recipe: QuantizationRecipe\n",
    "    train_recipe: TrainRecipe\n",
    "\n",
    "    def create_config_file(self, file_name):\n",
    "        \"\"\"\n",
    "        Creates a file with the given name and writes its contents to it in JSON format.\n",
    "\n",
    "        Args:\n",
    "            file_name (str): The name of the file.\n",
    "            dataclass_instance (MyClass): An instance of MyClass that contains the data to be written.\n",
    "        \"\"\"\n",
    "        \n",
    "        for fi in fields(self):\n",
    "            for fii in fields(fi.type):\n",
    "                print(fii)\n",
    "        return\n",
    "        # Get the path of the parent directory\n",
    "        parent_dir = os.path.dirname(os.path.dirname(__file__))\n",
    "\n",
    "        # Construct the full path to the target folder\n",
    "        target_folder_path = os.path.join(parent_dir, 'recipes', 'train')\n",
    "\n",
    "        # Create the target folder if it doesn't exist\n",
    "        if not os.path.exists(target_folder_path):\n",
    "            os.makedirs(target_folder_path)\n",
    "        \n",
    "        # Get the full path of the file\n",
    "        file_path = os.path.join(target_folder_path, file_name)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dataset_load']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x.name for x in fields(DatasetRecipe)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Field(name='dataset_load',type=typing.Optional[dict],default=None,default_factory=<dataclasses._MISSING_TYPE object at 0x000002B0FA1A9600>,init=True,repr=True,hash=None,compare=True,metadata=mappingproxy({'description': 'Kwargs for dataset load.'}),kw_only=False,_field_type=_FIELD),)\n",
      "(Field(name='model_load',type=typing.Optional[dict],default=None,default_factory=<dataclasses._MISSING_TYPE object at 0x000002B0FA1A9600>,init=True,repr=True,hash=None,compare=True,metadata=mappingproxy({'description': 'Kwargs for model load.'}),kw_only=False,_field_type=_FIELD), Field(name='model_config',type=typing.Optional[dict],default=None,default_factory=<dataclasses._MISSING_TYPE object at 0x000002B0FA1A9600>,init=True,repr=True,hash=None,compare=True,metadata=mappingproxy({'description': 'Kwargs for model configuration.'}),kw_only=False,_field_type=_FIELD))\n",
      "Field(name='DatasetRecipe',type=<class 'str'>,default=None,default_factory=<dataclasses._MISSING_TYPE object at 0x000002B0FA1A9600>,init=True,repr=True,hash=None,compare=True,metadata=mappingproxy({'required': True, 'description': \"Recipe's name from cookbook\", 'cookbook': '', 'recipe_keywords': mappingproxy({'__module__': 'recipes.datasets', '__annotations__': {'dataset_load': typing.Optional[dict]}, '__doc__': 'Kwargs to give to the \"load_dataset\" function from \"datasets\" module', 'dataset_load': None, 'preprocess_dataset': <function DatasetRecipe.preprocess_dataset at 0x000002B0AD80EA70>, 'preprocess_function': <function DatasetRecipe.preprocess_function at 0x000002B0AD80ECB0>, '__dataclass_params__': _DataclassParams(init=True,repr=True,eq=True,order=False,unsafe_hash=False,frozen=False), '__dataclass_fields__': {'dataset_load': Field(name='dataset_load',type=typing.Optional[dict],default=None,default_factory=<dataclasses._MISSING_TYPE object at 0x000002B0FA1A9600>,init=True,repr=True,hash=None,compare=True,metadata=mappingproxy({'description': 'Kwargs for dataset load.'}),kw_only=False,_field_type=_FIELD)}, '__init__': <function DatasetRecipe.__init__ at 0x000002B0AD80F130>, '__repr__': <function DatasetRecipe.__repr__ at 0x000002B0AD80F0A0>, '__eq__': <function DatasetRecipe.__eq__ at 0x000002B0AD80F2E0>, '__hash__': None, '__match_args__': ('dataset_load',)})}),kw_only=False,_field_type=_FIELD)\n",
      "Field(name='dataset_load',type=typing.Optional[dict],default=None,default_factory=<dataclasses._MISSING_TYPE object at 0x000002B0FA1A9600>,init=True,repr=True,hash=None,compare=True,metadata=mappingproxy({'description': 'Kwargs for dataset load.'}),kw_only=False,_field_type=_FIELD)\n",
      "Field(name='ModelRecipe',type=<class 'str'>,default=None,default_factory=<dataclasses._MISSING_TYPE object at 0x000002B0FA1A9600>,init=True,repr=True,hash=None,compare=True,metadata=mappingproxy({'required': True, 'description': \"Recipe's name from cookbook\", 'cookbook': '', 'recipe_keywords': mappingproxy({'__module__': 'recipes.models', '__annotations__': {'model_load': typing.Optional[dict], 'model_config': typing.Optional[dict]}, 'model_load': None, 'model_config': None, '__doc__': 'ModelRecipe(model_load: Optional[dict] = None, model_config: Optional[dict] = None)', '__dataclass_params__': _DataclassParams(init=True,repr=True,eq=True,order=False,unsafe_hash=False,frozen=False), '__dataclass_fields__': {'model_load': Field(name='model_load',type=typing.Optional[dict],default=None,default_factory=<dataclasses._MISSING_TYPE object at 0x000002B0FA1A9600>,init=True,repr=True,hash=None,compare=True,metadata=mappingproxy({'description': 'Kwargs for model load.'}),kw_only=False,_field_type=_FIELD), 'model_config': Field(name='model_config',type=typing.Optional[dict],default=None,default_factory=<dataclasses._MISSING_TYPE object at 0x000002B0FA1A9600>,init=True,repr=True,hash=None,compare=True,metadata=mappingproxy({'description': 'Kwargs for model configuration.'}),kw_only=False,_field_type=_FIELD)}, '__init__': <function ModelRecipe.__init__ at 0x000002B0AD80F5B0>, '__repr__': <function ModelRecipe.__repr__ at 0x000002B0AD80F520>, '__eq__': <function ModelRecipe.__eq__ at 0x000002B0AD80F760>, '__hash__': None, '__match_args__': ('model_load', 'model_config')})}),kw_only=False,_field_type=_FIELD)\n",
      "Field(name='model_load',type=typing.Optional[dict],default=None,default_factory=<dataclasses._MISSING_TYPE object at 0x000002B0FA1A9600>,init=True,repr=True,hash=None,compare=True,metadata=mappingproxy({'description': 'Kwargs for model load.'}),kw_only=False,_field_type=_FIELD)\n",
      "Field(name='model_config',type=typing.Optional[dict],default=None,default_factory=<dataclasses._MISSING_TYPE object at 0x000002B0FA1A9600>,init=True,repr=True,hash=None,compare=True,metadata=mappingproxy({'description': 'Kwargs for model configuration.'}),kw_only=False,_field_type=_FIELD)\n"
     ]
    }
   ],
   "source": [
    "fiis = []\n",
    "for fi in [DatasetRecipe, ModelRecipe]:\n",
    "    recipe_field = field(default=None, metadata = {\"required\": True, \n",
    "                                                    \"description\": \"Recipe's name from cookbook\",\n",
    "                                                    \"cookbook\": \"\", \n",
    "                                                    \"recipe_keywords\": vars(fi)})\n",
    "    fiis.append((fi.__name__, str, recipe_field))\n",
    "    print(fields(fi))\n",
    "    for fii in fields(fi):\n",
    "        fiis.append((fii.name, fii.type, fii))\n",
    "        \n",
    "t = make_dataclass(\"Test\", fiis)\n",
    "\n",
    "for fii in fields(t):\n",
    "    print(fii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Field(name='model_load',type=typing.Optional[dict],default=None,default_factory=<dataclasses._MISSING_TYPE object at 0x00000202BD1E9600>,init=True,repr=True,hash=None,compare=True,metadata=mappingproxy({'description': 'Kwargs for model load.'}),kw_only=False,_field_type=_FIELD)\n",
      "Field(name='model_config',type=typing.Optional[dict],default=None,default_factory=<dataclasses._MISSING_TYPE object at 0x00000202BD1E9600>,init=True,repr=True,hash=None,compare=True,metadata=mappingproxy({'description': 'Kwargs for model configuration.'}),kw_only=False,_field_type=_FIELD)\n",
      "Field(name='tokenizer_load',type=typing.Optional[dict],default=None,default_factory=<dataclasses._MISSING_TYPE object at 0x00000202BD1E9600>,init=True,repr=True,hash=None,compare=True,metadata=mappingproxy({'description': 'Kwargs for tokenizer load.'}),kw_only=False,_field_type=_FIELD)\n",
      "Field(name='tokenizer_config',type=typing.Optional[dict],default=None,default_factory=<dataclasses._MISSING_TYPE object at 0x00000202BD1E9600>,init=True,repr=True,hash=None,compare=True,metadata=mappingproxy({'description': 'Kwargs for tokenizer configuration.'}),kw_only=False,_field_type=_FIELD)\n",
      "Field(name='tokenizer_system',type=typing.Optional[str],default=None,default_factory=<dataclasses._MISSING_TYPE object at 0x00000202BD1E9600>,init=True,repr=True,hash=None,compare=True,metadata=mappingproxy({'description': 'System messsage to use.'}),kw_only=False,_field_type=_FIELD)\n",
      "Field(name='tokenizer_chat_template',type=typing.Optional[str],default=None,default_factory=<dataclasses._MISSING_TYPE object at 0x00000202BD1E9600>,init=True,repr=True,hash=None,compare=True,metadata=mappingproxy({'description': 'Chat template to use for prompt generation.'}),kw_only=False,_field_type=_FIELD)\n",
      "Field(name='peft_config_obj',type=typing.Optional[str],default=<property object at 0x000002028A2FA660>,default_factory=<dataclasses._MISSING_TYPE object at 0x00000202BD1E9600>,init=True,repr=True,hash=None,compare=True,metadata=mappingproxy({}),kw_only=False,_field_type=_FIELD)\n",
      "Field(name='peft_config',type=typing.Optional[dict],default=None,default_factory=<dataclasses._MISSING_TYPE object at 0x00000202BD1E9600>,init=True,repr=True,hash=None,compare=True,metadata=mappingproxy({'description': 'Kwargs for peft configuration.'}),kw_only=False,_field_type=_FIELD)\n",
      "Field(name='quantization_config',type=typing.Optional[dict],default=None,default_factory=<dataclasses._MISSING_TYPE object at 0x00000202BD1E9600>,init=True,repr=True,hash=None,compare=True,metadata=mappingproxy({'description': 'Kwargs for quantization configuration.'}),kw_only=False,_field_type=_FIELD)\n",
      "Field(name='dataset_name',type=typing.Optional[str],default=None,default_factory=<dataclasses._MISSING_TYPE object at 0x00000202BD1E9600>,init=True,repr=True,hash=None,compare=True,metadata=mappingproxy({'description': 'Huggingface path or local path of the dataset.', 'required': True}),kw_only=False,_field_type=_FIELD)\n",
      "Field(name='model_name',type=typing.Optional[str],default=None,default_factory=<dataclasses._MISSING_TYPE object at 0x00000202BD1E9600>,init=True,repr=True,hash=None,compare=True,metadata=mappingproxy({'description': 'Huggingface path or local path of the model.', 'required': True}),kw_only=False,_field_type=_FIELD)\n",
      "Field(name='tokenizer_name',type=typing.Optional[str],default=None,default_factory=<dataclasses._MISSING_TYPE object at 0x00000202BD1E9600>,init=True,repr=True,hash=None,compare=True,metadata=mappingproxy({'description': 'Huggingface path or local path of the tokenizer.', 'required': True}),kw_only=False,_field_type=_FIELD)\n",
      "Field(name='finetuner_arguments',type=typing.Optional[dict],default=None,default_factory=<dataclasses._MISSING_TYPE object at 0x00000202BD1E9600>,init=True,repr=True,hash=None,compare=True,metadata=mappingproxy({'description': 'Kwargs for finetuner configuration. For more information check out trl.SFTTrainer for available arguments.'}),kw_only=False,_field_type=_FIELD)\n",
      "Field(name='dataset_size',type=typing.Optional[int],default=None,default_factory=<dataclasses._MISSING_TYPE object at 0x00000202BD1E9600>,init=True,repr=True,hash=None,compare=True,metadata=mappingproxy({'description': 'Number of samples to limit the training dataset to (the validation_split_size of this will be taken if specified). Default: None -> No limits'}),kw_only=False,_field_type=_FIELD)\n",
      "Field(name='training_arguments',type=typing.Optional[dict],default=None,default_factory=<dataclasses._MISSING_TYPE object at 0x00000202BD1E9600>,init=True,repr=True,hash=None,compare=True,metadata=mappingproxy({'description': 'Training arguments to pass to the trainer. For more information check out transformers.TrainingArguments for available arguments.'}),kw_only=False,_field_type=_FIELD)\n",
      "Field(name='num_examples',type=<class 'int'>,default=0,default_factory=<dataclasses._MISSING_TYPE object at 0x00000202BD1E9600>,init=True,repr=True,hash=None,compare=True,metadata=mappingproxy({'description': 'Number of examples to provide for each prompt.'}),kw_only=False,_field_type=_FIELD)\n",
      "Field(name='completion_only',type=<class 'bool'>,default=True,default_factory=<dataclasses._MISSING_TYPE object at 0x00000202BD1E9600>,init=True,repr=True,hash=None,compare=True,metadata=mappingproxy({'description': 'Whether to train the model only on completion (i.e. text after response template) or the full prompt.'}),kw_only=False,_field_type=_FIELD)\n",
      "Field(name='shuffle',type=<class 'bool'>,default=True,default_factory=<dataclasses._MISSING_TYPE object at 0x00000202BD1E9600>,init=True,repr=True,hash=None,compare=True,metadata=mappingproxy({'description': 'Whether to shuffle the datasets.'}),kw_only=False,_field_type=_FIELD)\n",
      "Field(name='shuffle_seed',type=<class 'bool'>,default=42,default_factory=<dataclasses._MISSING_TYPE object at 0x00000202BD1E9600>,init=True,repr=True,hash=None,compare=True,metadata=mappingproxy({'description': \"Seed to use for the datasets' shuffle.\"}),kw_only=False,_field_type=_FIELD)\n",
      "Field(name='prompt_tuning_most_common_init',type=<class 'bool'>,default=False,default_factory=<dataclasses._MISSING_TYPE object at 0x00000202BD1E9600>,init=True,repr=True,hash=None,compare=True,metadata=mappingproxy({'description': 'Whether to use the most common words in the dataset as starting point for prompt tuning.'}),kw_only=False,_field_type=_FIELD)\n",
      "Field(name='num_proc',type=<class 'int'>,default=None,default_factory=<dataclasses._MISSING_TYPE object at 0x00000202BD1E9600>,init=True,repr=True,hash=None,compare=True,metadata=mappingproxy({'description': 'How many processes to use for dataset mapping.'}),kw_only=False,_field_type=_FIELD)\n",
      "Field(name='verbose',type=<class 'bool'>,default=False,default_factory=<dataclasses._MISSING_TYPE object at 0x00000202BD1E9600>,init=True,repr=True,hash=None,compare=True,metadata=mappingproxy({'description': 'Whether to set the logger to DEBUG mode.'}),kw_only=False,_field_type=_FIELD)\n"
     ]
    }
   ],
   "source": [
    "Config(None, None, None, None, None, None).create_config_file(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List\n",
    "from dataclasses import dataclass, make_dataclass, field, fields\n",
    "from recipes.Recipe import Recipe\n",
    "from recipes import DatasetRecipe, ModelRecipe\n",
    "\n",
    "\n",
    "def t(name: str, recipes: List[Recipe]):\n",
    "    fiis = []\n",
    "    for recipe in recipes:\n",
    "        recipe_field = field(default=None, metadata = {\"required\": True, \n",
    "                                                        \"description\": \"Recipe's name from cookbook\",\n",
    "                                                        \"cookbook\": \"\", \n",
    "                                                        \"recipe_keywords\": [x.name for x in fields(recipe)]})\n",
    "        fiis.append((recipe.__name__, str, recipe_field))\n",
    "        for fii in fields(recipe):\n",
    "            fiis.append((fii.name, fii.type, fii))\n",
    "    return make_dataclass(name, fiis)\n",
    "            \n",
    "Config = t(\"Config\", [DatasetRecipe, ModelRecipe])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Field(name='DatasetRecipe',type=<class 'str'>,default=None,default_factory=<dataclasses._MISSING_TYPE object at 0x000002B0FA1A9600>,init=True,repr=True,hash=None,compare=True,metadata=mappingproxy({'required': True, 'description': \"Recipe's name from cookbook\", 'cookbook': '', 'recipe_keywords': ['dataset_load']}),kw_only=False,_field_type=_FIELD),\n",
       " Field(name='dataset_load',type=typing.Optional[dict],default=None,default_factory=<dataclasses._MISSING_TYPE object at 0x000002B0FA1A9600>,init=True,repr=True,hash=None,compare=True,metadata=mappingproxy({'description': 'Kwargs for dataset load.'}),kw_only=False,_field_type=_FIELD),\n",
       " Field(name='ModelRecipe',type=<class 'str'>,default=None,default_factory=<dataclasses._MISSING_TYPE object at 0x000002B0FA1A9600>,init=True,repr=True,hash=None,compare=True,metadata=mappingproxy({'required': True, 'description': \"Recipe's name from cookbook\", 'cookbook': '', 'recipe_keywords': ['model_load', 'model_config']}),kw_only=False,_field_type=_FIELD),\n",
       " Field(name='model_load',type=typing.Optional[dict],default=None,default_factory=<dataclasses._MISSING_TYPE object at 0x000002B0FA1A9600>,init=True,repr=True,hash=None,compare=True,metadata=mappingproxy({'description': 'Kwargs for model load.'}),kw_only=False,_field_type=_FIELD),\n",
       " Field(name='model_config',type=typing.Optional[dict],default=None,default_factory=<dataclasses._MISSING_TYPE object at 0x000002B0FA1A9600>,init=True,repr=True,hash=None,compare=True,metadata=mappingproxy({'description': 'Kwargs for model configuration.'}),kw_only=False,_field_type=_FIELD))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fields(Config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 0, 'b': 'BTest', 't': 'test'}\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import asdict, is_dataclass, dataclass, fields\n",
    "\n",
    "@dataclass\n",
    "class BTest:\n",
    "    t: str\n",
    "\n",
    "@dataclass\n",
    "class A:\n",
    "    a: int\n",
    "    b: BTest\n",
    "    \n",
    "c = A(0, BTest(\"test\"))\n",
    "\n",
    "def config_to_flat_dict(config) -> dict:\n",
    "    config_dict = {}\n",
    "    for fi in fields(config):\n",
    "        if is_dataclass(fi.type):\n",
    "            config_dict[fi.name] = getattr(config, fi.name).__class__.__name__\n",
    "            config_dict.update(config_to_flat_dict(getattr(config, fi.name)))\n",
    "        else: \n",
    "            config_dict[fi.name] = getattr(config, fi.name)\n",
    "    return config_dict\n",
    "        \n",
    "print(config_to_flat_dict(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from recipes.pefts import LoRaPeftRecipe\n",
    "\n",
    "LoRaPeftRecipe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dataclasses import field\n",
    "\n",
    "\n",
    "a = field(default_factory=dict)\n",
    "\n",
    "a.default_factory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 1, 0, 1, 1, 0, 1, 0, 0]\n",
      "[0, 0, 1, 0, 0, 1, 0, 1, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"glue\", \"mrpc\", split=\"train\")\n",
    "\n",
    "dataset_support = dataset.select(range(len(dataset)))\n",
    "print(dataset[\"label\"][:10])\n",
    "\n",
    "shuffled_dataset = dataset.shuffle(seed=42)\n",
    "print(shuffled_dataset[\"label\"][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 1, 0, 1, 1, 0, 1, 0, 0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_support[\"label\"][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from recipes import DatasetRecipe\n",
    "from recipes.datasets.PostOCRCorrectionDatasetRecipe import PostOCRCorrectionDatasetRecipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response_template(tokenizer: PreTrainedTokenizer, dataset_recipe: DatasetRecipe) -> str:\n",
    "    if dataset_recipe.dataset_system_message is not None: \n",
    "        conversation = [{\"role\": \"system\", \"content\": dataset_recipe.dataset_system_message}]\n",
    "    else: conversation = []\n",
    "    \n",
    "    conversation_no_generation = conversation_generation = conversation + [{\"role\": \"user\", \"content\": \"dummy\"}]\n",
    "    dummy_no_generation_prompt = tokenizer.apply_chat_template(\n",
    "            conversation_no_generation,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=False,\n",
    "    )\n",
    "\n",
    "    dummy_generation_prompt = tokenizer.apply_chat_template(\n",
    "            conversation_generation,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True,\n",
    "    )\n",
    "    if len(dummy_no_generation_prompt) != len(dummy_generation_prompt):  \n",
    "        return dummy_generation_prompt[len(dummy_no_generation_prompt):]\n",
    "        \n",
    "    conversation_no_generation = conversation + [{\"role\": \"user\", \"content\": \"dummy\"}]\n",
    "    conversation_generation = conversation + [{\"role\": \"user\", \"content\": \"ymmud\"}]\n",
    "    dummy_no_generation_prompt = tokenizer.apply_chat_template(\n",
    "            conversation_no_generation,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=False,\n",
    "    )\n",
    "    \n",
    "    dummy_generation_prompt = tokenizer.apply_chat_template(\n",
    "            conversation_generation,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True,\n",
    "    )\n",
    "\n",
    "    for index, (c1, c2) in enumerate(zip(dummy_no_generation_prompt[::-1], \n",
    "                                        dummy_generation_prompt[::-1])):\n",
    "        if c1 != c2: return dummy_generation_prompt[-index:]\n",
    "    raise Exception(\"An error has occured during the creation of the response template. If the problem persists, set completion_only to False.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[/INST]'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_response_template(tokenizer, PostOCRCorrectionDatasetRecipe)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prompteffectiveness",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
